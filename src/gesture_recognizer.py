import cv2
import mediapipe as mp
import numpy as np
import joblib
from system_control import volume_up, volume_down, scroll_up, scroll_down

# è¼‰å…¥æ¨¡å‹èˆ‡æ¨™ç±¤ç·¨ç¢¼å™¨
knn = joblib.load("knn_model.joblib")
label_encoder = joblib.load("label_encoder.joblib")

# åˆå§‹åŒ– Mediapipe
mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils
hands = mp_hands.Hands(max_num_hands=1, min_detection_confidence=0.7)

# å•Ÿå‹•æ”å½±æ©Ÿ
cap = cv2.VideoCapture(0)
print("ğŸ– é–‹å§‹å³æ™‚æ‰‹å‹¢è¾¨è­˜ï¼ŒæŒ‰ä¸‹ 'q' éµçµæŸ")

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # ç¿»è½‰ç•«é¢èˆ‡è½‰æ›æ ¼å¼
    frame = cv2.flip(frame, 1)
    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    results = hands.process(rgb)

    gesture_name = "No Hand"

    if results.multi_hand_landmarks:
        hand_landmarks = results.multi_hand_landmarks[0]
        mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

        # å–å¾— landmark ç‰¹å¾µ
        landmarks = hand_landmarks.landmark
        x = [lm.x for lm in landmarks]
        y = [lm.y for lm in landmarks]
        z = [lm.z for lm in landmarks]
        feature = np.array(x + y + z).reshape(1, -1)

        # é æ¸¬æ‰‹å‹¢é¡åˆ¥
        prediction = knn.predict(feature)
        gesture_name = label_encoder.inverse_transform(prediction)[0]

        # æ ¹æ“šæ‰‹å‹¢åŸ·è¡Œå°æ‡‰æ“ä½œ
        if gesture_name == "volume_up":
            volume_up()
        elif gesture_name == "volume_down":
            volume_down()
        elif gesture_name == "scroll_up":
            scroll_up()
        elif gesture_name == "scroll_down":
            scroll_down()

    # é¡¯ç¤ºè¾¨è­˜çµæœ
    cv2.putText(frame, f"Gesture: {gesture_name}", (10, 40),
                cv2.FONT_HERSHEY_SIMPLEX, 1.2, (0, 255, 0), 3)
    cv2.imshow("Gesture Recognizer", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

# é—œé–‰æ”å½±æ©Ÿ
cap.release()
cv2.destroyAllWindows()
